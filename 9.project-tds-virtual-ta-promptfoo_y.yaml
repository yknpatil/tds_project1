description: "TDS Virtual TA Project Sample (but not the actual evaluation) Questions"

# This 'providers' section defines how your *main API* (your project's TA bot) is called.
# This should point to your locally running API or whatever endpoint your TA bot is exposed on.
providers:
  - id: https
    config:
      url: http://localhost:8000/api/ # Keep this as your project's API endpoint
      method: POST
      headers:
        Content-Type: application/json
        Authorization: Bearer {{ LLM_API_TOKEN }} # If your project's API requires a token
      body: |
        {
          "question": "{{ question }}"
          {% if image %}, "image": "{{ image }}"{% endif %}
        }
      transformResponse: json

# This 'defaultTest' section configures the LLM that `promptfoo` uses for the 'llm-rubric' assertions.
# It uses the AI Proxy endpoint as instructed.
defaultTest: # <--- This is now directly under the 'providers' block
  options:
    provider:
      id: https
      config:
        # This is the crucial part: use the AI Proxy URL for LLM rubric evaluations
        url: https://aiproxy.sanand.workers.dev/openai/v1/chat/completions
        method: POST
        headers:
          Content-Type: application/json
          Authorization: Bearer {{ AIPROXY_TOKEN }} # YOUR AI Proxy token goes here
        body: |
          {
            "model": "gpt-4o-mini",
            "messages": [
              {"role": "system", "content": "You are an evaluator that checks if an output meets specific criteria. Analyze the output based on the given rubric and respond with a JSON object containing {\"reason\": \"your analysis\", \"score\": number between 0.0 and 1.0, \"pass\": true/false}."},
              {"role": "user", "content": "Output to evaluate: {{ output }}\n\nRubric: {{ rubric }}"}
            ],
            "temperature": 0
          }
        transformResponse: json

  # This part ensures the overall response from your main API matches the expected JSON schema
  assert:
    - type: is-json
      value:
        type: object
        required: [answer, links]
        properties:
          answer: { type: string }
          links:
            type: array
            items:
              type: object
              required: [url, text]
              properties:
                url: { type: string }
                text: { type: string }

# Your individual tests for your project's TA bot
tests:
  - vars:
      prompt: "Evaluate the answer to the question: {{ question }}"
      question: The question asks to use gpt-3.5-turbo-0125 model but the ai-proxy provided by Anand sir only supports gpt-4o-mini. So should we just use gpt-4o-mini or use the OpenAI API for gpt3.5 turbo?
      image: file://project-tds-virtual-ta-q1.webp
      link: https://discourse.onlinedegree.iitm.ac.in/t/ga5-question-8-clarification/155939
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Clarifies use of gpt-3.5-turbo-0125 not gpt-4o-mini
      - type: contains
        transform: JSON.stringify(output.links)
        value: https://discourse.onlinedegree.iitm.ac.in/t/ga5-question-8-clarification/155939

  - vars:
      prompt: "Evaluate the answer to the question: {{ question }}"
      question: If a student scores 10/10 on GA4 as well as a bonus, how would it appear on the dashboard?
      link: https://discourse.onlinedegree.iitm.ac.in/t/ga4-data-sourcing-discussion-thread-tds-jan-2025/165959/388
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Mentions the dashboard showing "110"
      - type: contains
        transform: JSON.stringify(output.links)
        value: https://discourse.onlinedegree.iitm.ac.in/t/ga4-data-sourcing-discussion-thread-tds-jan-2025/165959

  - vars:
      prompt: "Evaluate the answer to the question: {{ question }}"
      question: I know Docker but have not used Podman before. Should I use Docker for this course?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Recommends Podman for the course
      - type: llm-rubric
        transform: output.answer
        value: Mentions that Docker is acceptable
      - type: contains
        transform: JSON.stringify(output.links)
        value: https://tds.s-anand.net/#/docker

  - vars:
      prompt: "Evaluate the answer to the question: {{ question }}"
      question: When is the TDS Sep 2025 end-term exam?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Says it doesn't know (since this information is not available yet)

writeLatestResults: true

commandLineOptions:
  cache: true